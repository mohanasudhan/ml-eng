{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Embedding?\n",
    "\n",
    "Embedding converts categories into continuous vectors\n",
    "\n",
    "Embedding, mainly used in deep learning, converts categorical data into dense vectors of continuous numbers. These vectors capture semantic relationships between categories and reduce dimensionality efficiently. Pre-trained embeddings like Word2Vec, GloVe, and FastText are available for tasks like natural language processing (NLP) and recommendation systems. Trainable embeddings are learned during model training for specific tasks.\n",
    "\n",
    "Advantages of embeddings include their compactness and ability to capture semantic relationships, improving model performance. However, embeddings are computationally expensive and less interpretable compared to encodings.\n",
    "\n",
    "The choice between encoding and embedding depends on the dataset size, number of categories, and specific problem requirements. One-hot encoding is suitable for small datasets with few categories, while embeddings are preferred for large datasets, especially in NLP tasks and recommendation systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Obtaining dependency information for gensim from https://files.pythonhosted.org/packages/63/46/5feab9c524a380bfa9f9f1c0d065743280dca30b216ab4c7a231f22dbed7/gensim-4.3.2-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading gensim-4.3.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/mohanasudhangandhi/.pyenv/versions/venv-3.11/lib/python3.11/site-packages (from gensim) (1.25.2)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Users/mohanasudhangandhi/.pyenv/versions/venv-3.11/lib/python3.11/site-packages (from gensim) (1.12.0)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Obtaining dependency information for smart-open>=1.8.1 from https://files.pythonhosted.org/packages/ad/08/dcd19850b79f72e3717c98b2088f8a24b549b29ce66849cd6b7f44679683/smart_open-7.0.1-py3-none-any.whl.metadata\n",
      "  Downloading smart_open-7.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting wrapt (from smart-open>=1.8.1->gensim)\n",
      "  Obtaining dependency information for wrapt from https://files.pythonhosted.org/packages/0f/16/ea627d7817394db04518f62934a5de59874b587b792300991b3c347ff5e0/wrapt-1.16.0-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading wrapt-1.16.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Downloading gensim-4.3.2-cp311-cp311-macosx_11_0_arm64.whl (24.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading smart_open-7.0.1-py3-none-any.whl (60 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wrapt-1.16.0-cp311-cp311-macosx_11_0_arm64.whl (38 kB)\n",
      "Installing collected packages: wrapt, smart-open, gensim\n",
      "Successfully installed gensim-4.3.2 smart-open-7.0.1 wrapt-1.16.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# The Gensim library provides tools and algorithms for topic modeling, \n",
    "# document similarity analysis, and other natural language processing (NLP) tasks.\n",
    "!pip install gensim\n",
    "\n",
    "# Full form of gensim - generate similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'machine': [ 0.1476101  -0.03066943 -0.09073226  0.13108103 -0.09720321]\n",
      "Embedding for sentence 'I love machine learning': [-0.01302523  0.02925177  0.0208698   0.0414466  -0.10625307]\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample data\n",
    "sentences = [['I', 'love', 'machine', 'learning'],\n",
    "             ['machine', 'learning', 'is', 'awesome'],\n",
    "             ['deep', 'learning', 'is', 'interesting']]\n",
    "\n",
    "# Training Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=5, window=3, min_count=1, sg=1)\n",
    "\n",
    "# Getting embedding for a word\n",
    "word_embedding = model.wv['machine']\n",
    "print(\"Embedding for 'machine':\", word_embedding)\n",
    "\n",
    "# Getting embedding for a sentence\n",
    "sentence_embedding = np.mean([model.wv[word] for word in sentences[0]], axis=0)\n",
    "print(\"Embedding for sentence 'I love machine learning':\", sentence_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Obtaining dependency information for nltk from https://files.pythonhosted.org/packages/a6/0a/0d20d2c0f16be91b9fa32a77b76c60f9baf6eba419e5ef5deca17af9c582/nltk-3.8.1-py3-none-any.whl.metadata\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting click (from nltk)\n",
      "  Obtaining dependency information for click from https://files.pythonhosted.org/packages/00/2e/d53fa4befbf2cfa713304affc7ca780ce4fc1fd8710527771b58311a3229/click-8.1.7-py3-none-any.whl.metadata\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: joblib in /Users/mohanasudhangandhi/.pyenv/versions/venv-3.11/lib/python3.11/site-packages (from nltk) (1.3.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Obtaining dependency information for regex>=2021.8.3 from https://files.pythonhosted.org/packages/60/9e/4b0223e05776aa3be806a902093b2ab1de3ba26b652d92065d5c7e1d4df3/regex-2023.12.25-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading regex-2023.12.25-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tqdm (from nltk)\n",
      "  Obtaining dependency information for tqdm from https://files.pythonhosted.org/packages/2a/14/e75e52d521442e2fcc9f1df3c5e456aead034203d4797867980de558ab34/tqdm-4.66.2-py3-none-any.whl.metadata\n",
      "  Downloading tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.12.25-cp311-cp311-macosx_11_0_arm64.whl (291 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m291.0/291.0 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, regex, click, nltk\n",
      "Successfully installed click-8.1.7 nltk-3.8.1 regex-2023.12.25 tqdm-4.66.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# In the above example words are already tokenized. However it will not be the case always\n",
    "# In such case you can use tokenizer libraries\n",
    "! pip install nltk\n",
    "\n",
    "# nl - natural language toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/mohanasudhangandhi/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The 'punkt' tokenizer is a pre-trained model used by NLTK for tokenizing sentences. \n",
    "# It's not included by default when you install NLTK, so you need to download it separately.\n",
    "\n",
    "# You can download it using the NLTK downloader. \n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Embeddings:\n",
      "machine: [ 0.00278317  0.04964359  0.07698309 -0.01144223  0.04323421 -0.05814379\n",
      " -0.00804191  0.08100051 -0.02360065 -0.09663455]\n",
      "learning: [-0.037098   -0.08745642  0.05437467  0.06509756 -0.0078755  -0.06709856\n",
      " -0.07085925 -0.0249706   0.05143254 -0.03665238]\n",
      "is: [-0.00547545  0.00242737  0.05115977  0.09006381 -0.09300154 -0.07114856\n",
      "  0.06470766  0.0896343  -0.05010227 -0.03771948]\n",
      "the: [-0.08531428  0.03201014 -0.04636526 -0.0510409   0.03596911  0.05383289\n",
      "  0.07779722 -0.05787721  0.07406821  0.06628729]\n",
      "scientific: [ 0.07817571 -0.09510187 -0.00205531  0.03469197 -0.00938972  0.08381772\n",
      "  0.09010784  0.06536506 -0.00711621  0.07710405]\n",
      "study: [ 0.07913878 -0.07002877 -0.09149235 -0.00370559 -0.0311619   0.07894032\n",
      "  0.05949505 -0.01549919  0.01477904  0.01776185]\n",
      "of: [ 0.07299155  0.05040807  0.06833922  0.00699987  0.06393988 -0.03357086\n",
      " -0.00889996  0.05728829 -0.0760235  -0.03941352]\n",
      "algorithms: [ 0.02343239 -0.04517407  0.08408428 -0.09853405  0.0676281   0.02912856\n",
      " -0.0491908   0.04403302 -0.01748836  0.06706805]\n",
      "and: [-0.0960355   0.05007293 -0.08759586 -0.04391825 -0.000351   -0.00296181\n",
      " -0.0766124   0.09614743  0.04982058  0.09233143]\n",
      "statistical: [ 0.0995965  -0.04373094 -0.00566537 -0.05733141  0.03840852  0.02800288\n",
      "  0.06946848  0.06073941  0.09491776  0.09254427]\n",
      "models: [-0.01577653  0.00321372 -0.0414063  -0.07682689 -0.01508008  0.02469795\n",
      " -0.00888027  0.05533662 -0.02742977  0.02260065]\n",
      "that: [-0.00404749 -0.08368222 -0.05592128  0.07098617  0.03356257  0.07228431\n",
      "  0.06803721  0.07528118 -0.03798491 -0.00562586]\n",
      "computer: [-0.08619688  0.03665738  0.05189884  0.05741938  0.07466918 -0.06167675\n",
      "  0.01105614  0.06047282 -0.0284005  -0.06173522]\n",
      "systems: [ 0.016519    0.00159352  0.03519901  0.00156121  0.09622756  0.0508589\n",
      " -0.08846101 -0.07060052  0.00808139  0.0637829 ]\n",
      "use: [-0.00845374  0.02823001  0.05420326  0.07025852 -0.05686755  0.01872147\n",
      "  0.06106435 -0.04814924 -0.03134002  0.06792384]\n",
      "to: [ 0.05455794  0.08345953 -0.01453741 -0.09208143  0.04370552  0.00571785\n",
      "  0.07441908 -0.00813283 -0.02638414 -0.08753009]\n",
      "perform: [-0.0935114   0.03794741  0.04910555 -0.06503135  0.01232206 -0.02036605\n",
      "  0.00068717 -0.09949803  0.02595191 -0.04761646]\n",
      "a: [-0.07522446 -0.00939525  0.09595003 -0.07343656 -0.0233041  -0.01932516\n",
      "  0.08113458 -0.05938404  0.00023576 -0.04781922]\n",
      "specific: [ 0.01073093 -0.01589807  0.02218165 -0.07904731 -0.02708323  0.02677422\n",
      "  0.05388363 -0.02416563 -0.09556816  0.0450408 ]\n",
      "task: [-0.04518094  0.05696851  0.09218399 -0.04113272  0.07981843  0.05384171\n",
      "  0.05888505  0.00506748  0.08201542 -0.07007322]\n",
      "without: [ 0.04284936  0.0006573  -0.09564884 -0.09689067 -0.06139309 -0.00107018\n",
      "  0.02032326  0.09417954  0.05551072 -0.04290984]\n",
      "using: [ 0.03429785  0.05166117  0.06282345 -0.02804263  0.07322703  0.02830272\n",
      "  0.02871004 -0.0238037  -0.0312825  -0.02370142]\n",
      "explicit: [ 0.00650274 -0.03844688 -0.07058729 -0.02149093  0.03967369  0.08857775\n",
      "  0.09303553 -0.06010947 -0.09447427  0.09771667]\n",
      "instructions: [ 0.01680265 -0.02211137  0.09522366  0.09453142 -0.09772427  0.02517943\n",
      "  0.06189521  0.038386    0.01981011  0.00420952]\n",
      ",: [ 0.09405217  0.04661077  0.03981214 -0.06260912  0.08449836 -0.0214384\n",
      "  0.08853785 -0.05362055 -0.08160572  0.06812586]\n",
      "relying: [-0.08242677  0.09299354 -0.00197661 -0.01967276  0.0460363  -0.04095316\n",
      "  0.02743114  0.06939967  0.06065426 -0.07510795]\n",
      "on: [ 0.01218818 -0.08458325 -0.08223945 -0.00231016  0.01237288 -0.05743381\n",
      " -0.04725274 -0.07346074  0.08328615  0.00121298]\n",
      "patterns: [-0.04637323 -0.03164107  0.09311356  0.00873386  0.07490703 -0.06074063\n",
      "  0.05160507  0.09922823 -0.08457391 -0.05135691]\n",
      "and: [-0.0960355   0.05007293 -0.08759586 -0.04391825 -0.000351   -0.00296181\n",
      " -0.0766124   0.09614743  0.04982058  0.09233143]\n",
      "inference: [ 4.0090930e-02  5.1853694e-02  4.2559016e-02  1.9397546e-02\n",
      " -3.1701624e-02  8.3538450e-02  9.6121803e-02  3.7926029e-02\n",
      " -2.8369952e-02  7.1275237e-05]\n",
      "instead: [ 0.0428607  -0.03757985  0.08399031  0.01495883 -0.07250727  0.09455959\n",
      "  0.07688485  0.05454434 -0.06909709  0.05810196]\n",
      ".: [ 0.07374393 -0.01545998 -0.04477603  0.06490683 -0.0484458  -0.01804097\n",
      "  0.02923506  0.00961574 -0.08342029 -0.09470666]\n",
      "it: [ 0.0142451  -0.02678653 -0.07008485 -0.07880285 -0.09075452 -0.05877251\n",
      " -0.01779589 -0.0437734  -0.06545505 -0.03695018]\n",
      "is: [-0.00547545  0.00242737  0.05115977  0.09006381 -0.09300154 -0.07114856\n",
      "  0.06470766  0.0896343  -0.05010227 -0.03771948]\n",
      "seen: [-0.05598493  0.01699293 -0.00872512  0.06746367  0.03995601  0.04572291\n",
      "  0.01494293 -0.02735786 -0.04426772 -0.01004506]\n",
      "as: [ 0.05452666 -0.07439248 -0.07360677 -0.02500684 -0.08625839 -0.01559678\n",
      " -0.00333196  0.03274547  0.01382747 -0.0090227 ]\n",
      "a: [-0.07522446 -0.00939525  0.09595003 -0.07343656 -0.0233041  -0.01932516\n",
      "  0.08113458 -0.05938404  0.00023576 -0.04781922]\n",
      "subset: [-0.07070359 -0.04872579 -0.03775686 -0.08554735  0.07947861 -0.04841448\n",
      "  0.08442261  0.05245322 -0.06568454  0.03953504]\n",
      "of: [ 0.07299155  0.05040807  0.06833922  0.00699987  0.06393988 -0.03357086\n",
      " -0.00889996  0.05728829 -0.0760235  -0.03941352]\n",
      "artificial: [ 0.0010331   0.03071344 -0.06805466 -0.01371733  0.07664855  0.07344286\n",
      " -0.03664194  0.02646181 -0.08341458  0.06192074]\n",
      "intelligence: [-0.08157917  0.04495798 -0.04137076  0.00824536  0.08498619 -0.04462177\n",
      "  0.045175   -0.0678696  -0.03548489  0.09398508]\n",
      ".: [ 0.07374393 -0.01545998 -0.04477603  0.06490683 -0.0484458  -0.01804097\n",
      "  0.02923506  0.00961574 -0.08342029 -0.09470666]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample text corpus\n",
    "text_corpus = \"Machine learning is the scientific study of algorithms and statistical models that computer systems use to perform a specific task without using explicit instructions, relying on patterns and inference instead. It is seen as a subset of artificial intelligence.\"\n",
    "\n",
    "# Tokenizing text\n",
    "tokens = word_tokenize(text_corpus.lower())  # Convert to lowercase for consistency\n",
    "\n",
    "# Training Word2Vec model\n",
    "# vector size - vector dimension\n",
    "\n",
    "# window - it determines the context size or the number of words considered before and after the target word \n",
    "# when training the Word2Vec model. In this example, window=5 means that the context window size is set to 5 words.\n",
    "\n",
    "# min_count - Words with frequencies lower than min_count will be ignored and not included in the vocabulary. \n",
    "# In this example, min_count=1 means that all words appearing at least once in the corpus will be included in the vocabulary.\n",
    "\n",
    "# sg=0 for the Continuous Bag of Words (CBOW) model and sg=1 for the Skip-gram model. \n",
    "# In this example, sg=1 indicates that the Skip-gram model will be used for training.\n",
    "model = Word2Vec([tokens], vector_size=10, window=5, min_count=1, sg=1)\n",
    "\n",
    "# Accessing word embeddings\n",
    "word_embeddings = model.wv\n",
    "\n",
    "# Displaying word embeddings for individual words\n",
    "print(\"Word Embeddings:\")\n",
    "for word in tokens:\n",
    "    print(f\"{word}: {word_embeddings[word]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-gram (sg) Vs CBOW\n",
    "\n",
    "Imagine you're trying to learn about a topic by asking your friends questions. The Skip-gram model operates similarly to this scenario.\n",
    "\n",
    "In the Skip-gram model:\n",
    "\n",
    "You are like the target word you're trying to learn more about.\n",
    "Your friends' responses are like the context words surrounding the target word.\n",
    "By asking your friends different questions (providing different target words), you can learn about the relationships between your target word and the words that often appear around it.\n",
    "So, in a sense, the Skip-gram model learns by predicting the context words given a target word. It tries to understand the meaning of a word by observing the words that tend to appear nearby in a sentence or text corpus.\n",
    "\n",
    "In contrast, the Continuous Bag of Words (CBOW) model works in the opposite way. Instead of predicting context words given a target word, it predicts a target word given a set of context words. You can think of CBOW as trying to guess the missing word in a sentence given the words surrounding it.\n",
    "\n",
    "Overall, the Skip-gram model is like learning from the context, while the CBOW model is like learning from the target itself. Both approaches have their advantages and are suitable for different types of text data and tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
